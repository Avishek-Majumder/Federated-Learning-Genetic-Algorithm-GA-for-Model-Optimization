{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d932c31-435a-4d04-8432-a2f54de906e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 1: Import Required Libraries and Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae79479-0932-4e1c-8d6b-77451b64032c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import cv2\n",
    "import os\n",
    "os.environ[\"PATH\"] += os.pathsep + r\"C:\\Program Files\\Graphviz\\bin\"\n",
    "from imutils import paths\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import pydot\n",
    "import graphviz\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "def load_image_paths(img_path):\n",
    "    return list(paths.list_images(img_path))\n",
    "\n",
    "def load_data(image_paths, verbose=-1):\n",
    "    data = []\n",
    "    labels = []\n",
    "    for (i, imgpath) in enumerate(image_paths):\n",
    "        im_gray = cv2.imread(imgpath, cv2.IMREAD_GRAYSCALE)\n",
    "        image = cv2.resize(im_gray, (96, 96))\n",
    "        label = imgpath.split(os.path.sep)[-2]\n",
    "        data.append(image / 255.0)\n",
    "        labels.append(label)\n",
    "        if verbose > 0 and i > 0 and (i + 1) % verbose == 0:\n",
    "            print(\"[INFO] processed {}/{}\".format(i + 1, len(image_paths)))\n",
    "    return data, labels\n",
    "\n",
    "def count_images(directory):\n",
    "    counts = {}\n",
    "    for label in os.listdir(directory):\n",
    "        counts[label] = len(os.listdir(os.path.join(directory, label)))\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0711528b-4b67-49e0-a988-d014463cede9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 2: Load Image Paths and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ffac13-363c-4d11-a8d1-6efed968c57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSES = [ 'NPD',\n",
    "            'PD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f3ce079-e200-4ee5-bca8-d6fa69d34c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 3: Print Shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45654e3-86b3-41ba-91f2-09f677ff5e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "# Define path to your data folder\n",
    "img_path = r'C:\\Users\\ArnaB\\Downloads\\Compressed\\ntua-parkinson-dataset-master_2\\NTUA\\FINAL DATASET(PREPROCESSED)\\train'\n",
    "\n",
    "# Get the path list using the path object\n",
    "image_paths = load_image_paths(img_path)\n",
    "\n",
    "# Apply our function\n",
    "image_list, label_list = load_data(image_paths, verbose=2)\n",
    "\n",
    "# Binarize the labels\n",
    "lb = LabelBinarizer()\n",
    "label_list = lb.fit_transform(label_list)\n",
    "\n",
    "# Ensure the labels are one-hot encoded correctly\n",
    "if label_list.shape[1] == 1:\n",
    "    label_list = np.hstack((1 - label_list, label_list))\n",
    "\n",
    "# Split data into training and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(image_list, label_list, test_size=0.1, random_state=42)\n",
    "\n",
    "print(\"Train Directory Contents:\", os.listdir(img_path))\n",
    "\n",
    "# Define classes\n",
    "CLASSES = ['NPD', 'PD']\n",
    "\n",
    "# Step 4: Print Shapes\n",
    "print(np.shape(X_train))  # Expected: (16900, 96, 96)\n",
    "print(np.shape(X_test))   # Expected: (1878, 96, 96)\n",
    "print(np.shape(y_train))  # Expected: (16900, 2)\n",
    "print(np.shape(y_test))   # Expected: (1878, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bfbd897-364e-4c7e-91d8-adf4e5f490b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 4: Convert One-Hot Labels to Single Labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9411b843-58a7-4168-8c0c-dec750c57baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert one-hot encoded labels to single labels\n",
    "y_train_single = np.argmax(y_train, axis=1)\n",
    "y_test_single = np.argmax(y_test, axis=1)\n",
    "\n",
    "print(np.shape(y_train_single))  # Should be (447,)\n",
    "print(np.shape(y_test_single))   # Should be (50,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6a1ba1-bbb5-4bd8-93a0-0429c79b96e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 5: Apply SMOTE with Reduced Number of Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b299990-07d1-46fb-8b07-939f2a0ebca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure X_train is a numpy array\n",
    "X_train = np.array(X_train)\n",
    "\n",
    "# Adjust k_neighbors based on your smallest class size\n",
    "sm = SMOTE(random_state=42, k_neighbors=1)\n",
    "X_train_resampled, y_train_resampled = sm.fit_resample(X_train.reshape(-1, 96*96), y_train_single)\n",
    "X_train_resampled = X_train_resampled.reshape(-1, 96, 96, 1)\n",
    "\n",
    "print(np.shape(X_train_resampled))  # Should be (new_size, 96, 96, 1)\n",
    "print(np.shape(y_train_resampled))  # Should be (new_size,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02001c6a-8ae8-48c9-8bd6-d958150b67ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 6: Convert Single Labels Back to One-Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef6931a-9227-4a16-b4c1-cae489d4fee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of classes\n",
    "num_classes = y_train.shape[1]  # Should be 71\n",
    "\n",
    "# Convert single labels back to one-hot encoding\n",
    "y_train_resampled_one_hot = np.eye(num_classes)[y_train_resampled]\n",
    "y_test_one_hot = np.eye(num_classes)[y_test_single]\n",
    "\n",
    "print(np.shape(y_train_resampled_one_hot))  # Should be (new_size, num_classes)\n",
    "print(np.shape(y_test_one_hot))            # Should be (50, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1ac2f4-941b-4426-b51c-1af0eca00ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 7: Create Clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677225f8-792e-417b-8046-e576c1e483d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_clients(image_list, label_list, num_clients=6, initial='clients'):\n",
    "    ''' return: a dictionary with keys clients' names and value as\n",
    "                data shards - tuple of images and label lists.\n",
    "        args:\n",
    "            image_list: a list of numpy arrays of training images\n",
    "            label_list: a list of binarized labels for each image\n",
    "            num_client: number of federated members (clients)\n",
    "            initials: the clients' name prefix, e.g., clients_1\n",
    "    '''\n",
    "\n",
    "    # create a list of client names\n",
    "    client_names = ['{}_{}'.format(initial, i+1) for i in range(num_clients)]\n",
    "\n",
    "    # randomize the data\n",
    "    data = list(zip(image_list, label_list))\n",
    "    random.shuffle(data)\n",
    "\n",
    "    # shard data and place at each client\n",
    "    size = len(data)//num_clients\n",
    "    shards = [data[i:i + size] for i in range(0, size*num_clients, size)]\n",
    "\n",
    "    # number of clients must equal the number of shards\n",
    "    assert(len(shards) == len(client_names))\n",
    "\n",
    "    return {client_names[i]: shards[i] for i in range(len(client_names))}\n",
    "\n",
    "# create clients\n",
    "clients = create_clients(X_train_resampled, y_train_resampled_one_hot, num_clients=6, initial='client')\n",
    "\n",
    "# Print the client names and the size of their data shards\n",
    "for client_name, data_shard in clients.items():\n",
    "    print(f\"{client_name}: {len(data_shard)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d440156-2e04-4018-9024-bdf05d79f7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 8: Batch Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97838ba-1d6a-4888-92b5-8c0004ffaabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_data(data_shard, bs=4):\n",
    "    '''Takes in a client's data shard and create a tfds object off it\n",
    "    args:\n",
    "        data_shard: a data, label constituting a client's data shard\n",
    "        bs: batch size\n",
    "    return:\n",
    "        tfds object'''\n",
    "    # separate shard into data and labels lists\n",
    "    data, label = zip(*data_shard)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((list(data), list(label)))\n",
    "    return dataset.shuffle(len(label)).batch(bs)\n",
    "\n",
    "# process and batch the training data for each client\n",
    "clients_batched = dict()\n",
    "for (client_name, data) in clients.items():\n",
    "    clients_batched[client_name] = batch_data(data)\n",
    "\n",
    "# process and batch the test set\n",
    "test_batched = tf.data.Dataset.from_tensor_slices((X_test, y_test_one_hot)).batch(len(y_test_one_hot))\n",
    "\n",
    "# Print the size of batched data for each client\n",
    "for client_name, data in clients_batched.items():\n",
    "    print(f\"{client_name}: {tf.data.experimental.cardinality(data).numpy()} batches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40759b63-e997-43f2-98ff-f431d2a2500d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 9: Define CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e78957-294c-40f7-a22e-2973a8231c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN:\n",
    "    @staticmethod\n",
    "    def build(width, height, channels, classes):\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Conv2D(filters=16, kernel_size=(3, 3), strides=(1, 1), activation=\"relu\", kernel_initializer='he_normal',\n",
    "                         input_shape=(width, height, channels)))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "        model.add(Conv2D(filters=32, kernel_size=(3, 3), strides=(1, 1), activation=\"relu\", kernel_initializer='he_normal'))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "        model.add(Conv2D(filters=128, kernel_size=(3, 3), strides=(1, 1), activation=\"relu\", kernel_initializer='he_normal'))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(128, activation=\"relu\", kernel_initializer='he_normal'))\n",
    "        model.add(Dense(64, activation=\"relu\"))\n",
    "        model.add(Dense(classes, activation=\"softmax\"))\n",
    "\n",
    "        model.compile(optimizer='adam', loss=\"categorical_crossentropy\", metrics=['accuracy'])\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c39603-f445-4012-b5fd-d85f7325b6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Step 10: Define the cnn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a2816b-15fa-4f44-a0eb-fe21e9189378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define CNN model\n",
    "model = CNN.build(96, 96, 1, 2)\n",
    "model.summary()  # Print the model summary to verify the structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16dd8189-15a6-4101-bfde-3223d2e2b98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Step 11: Train the Federated Model(Genetic Algorithm) For 10 generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798e1052-52bf-49aa-9704-a9934f0d2d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# GA Hyperparameters\n",
    "num_generations = 10\n",
    "population_size = len(clients_batched)  # Population size matches the number of clients\n",
    "num_parents = population_size // 2  # Number of top-performing parents each generation\n",
    "crossover_rate = 0.8  # Probability of crossover\n",
    "mutation_rate = 0.02  # Reduced mutation rate to limit drastic weight changes\n",
    "\n",
    "# Initialize global model weights\n",
    "global_weights = model.get_weights()\n",
    "\n",
    "# Compile the global model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Function to select parents based on fitness scores\n",
    "def select_parents(population, fitness_scores, num_parents):\n",
    "    parents = [population[i] for i in np.argsort(fitness_scores)[-num_parents:]]\n",
    "    return parents\n",
    "\n",
    "# Crossover function to blend weights of two parent models\n",
    "def crossover(parent1, parent2, crossover_rate=0.8):\n",
    "    child = tf.keras.models.clone_model(parent1)\n",
    "    child_weights = []\n",
    "    for w1, w2 in zip(parent1.get_weights(), parent2.get_weights()):\n",
    "        mask = np.random.rand(*w1.shape) < crossover_rate\n",
    "        child_weights.append(np.where(mask, w1, w2))\n",
    "    child.set_weights(child_weights)\n",
    "    return child\n",
    "\n",
    "# Mutation function to apply small random noise to the weights\n",
    "def mutate(model, mutation_rate=0.02):\n",
    "    mutated_weights = []\n",
    "    for w in model.get_weights():\n",
    "        if np.random.rand() < mutation_rate:\n",
    "            noise = np.random.normal(scale=0.005, size=w.shape)\n",
    "            mutated_weights.append(w + noise)\n",
    "        else:\n",
    "            mutated_weights.append(w)\n",
    "    model.set_weights(mutated_weights)\n",
    "    return model\n",
    "\n",
    "# Replacement function to replace less fit models with new offspring\n",
    "def replace_population(population, offspring, fitness_scores):\n",
    "    sorted_indices = np.argsort(fitness_scores)\n",
    "    num_replace = min(len(offspring), len(sorted_indices))\n",
    "    for i in range(num_replace):\n",
    "        population[sorted_indices[i]] = offspring[i]\n",
    "    return population\n",
    "\n",
    "# Initialize population with each client model\n",
    "population = [tf.keras.models.clone_model(model) for _ in range(population_size)]\n",
    "for individual in population:\n",
    "    individual.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "                       loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "                       metrics=['accuracy'])\n",
    "\n",
    "# Training with GA over multiple generations\n",
    "for generation in range(num_generations):\n",
    "    print(f\"Generation {generation + 1}/{num_generations}\")\n",
    "    \n",
    "    # Step 1: Evaluate fitness of each client model in the population\n",
    "    fitness_scores = []\n",
    "    for individual, (client_name, client_data) in zip(population, clients_batched.items()):\n",
    "        print(f\"Evaluating Client: {client_name}\")\n",
    "        \n",
    "        individual.set_weights(global_weights)  # Start from global weights\n",
    "        \n",
    "        # Compile model after setting weights\n",
    "        individual.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "                           loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "                           metrics=['accuracy'])\n",
    "        \n",
    "        for epoch in range(5):  # You can adjust the number of epochs here\n",
    "            for x_batch, y_batch in client_data:\n",
    "                individual.train_on_batch(x_batch, y_batch)\n",
    "        \n",
    "        _, accuracy = individual.evaluate(client_data, verbose=0)\n",
    "        fitness_scores.append(accuracy)\n",
    "    \n",
    "    # Step 2: Selection - Select top-performing parents\n",
    "    parents = select_parents(population, fitness_scores, num_parents)\n",
    "    \n",
    "    # Step 3: Crossover and Mutation - Generate offspring\n",
    "    offspring = []\n",
    "    for i in range(0, len(parents), 2):\n",
    "        parent1, parent2 = parents[i], parents[(i + 1) % len(parents)]\n",
    "        child = crossover(parent1, parent2, crossover_rate)\n",
    "        child = mutate(child, mutation_rate)\n",
    "        offspring.append(child)\n",
    "    \n",
    "    # Step 4: Replacement - Replace less fit individuals with new offspring\n",
    "    population = replace_population(population, offspring, fitness_scores)\n",
    "    \n",
    "    # Step 5: Update global weights as the average of top models\n",
    "    top_indices = np.argsort(fitness_scores)[-num_parents:]\n",
    "    top_weights = [population[i].get_weights() for i in top_indices]\n",
    "    new_global_weights = [np.mean([weights[layer] for weights in top_weights], axis=0) for layer in range(len(global_weights))]\n",
    "    global_weights = new_global_weights\n",
    "\n",
    "    # Log the average fitness of the generation for stability tracking\n",
    "    avg_fitness = np.mean([fitness_scores[i] for i in top_indices])\n",
    "    print(f\"Average top fitness in generation {generation + 1}: {avg_fitness}\")\n",
    "\n",
    "# After training, evaluate the global model on test data\n",
    "test_batched = test_batched.map(lambda x, y: (tf.expand_dims(x, -1), y))\n",
    "\n",
    "# Set global weights to model for final evaluation\n",
    "model.set_weights(global_weights)\n",
    "\n",
    "# Compile the model again before evaluating\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Evaluate the global model on the test data\n",
    "test_loss, test_accuracy = model.evaluate(test_batched, verbose=0)\n",
    "print(f\"Test loss: {test_loss}\")\n",
    "print(f\"Test accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfa1853-bbf6-4de5-ad42-cb5a2bfd1c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "Step 12: Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad01bf2-202c-43b9-a203-59238486cd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(r'C:\\Users\\ArnaB\\Downloads\\Compressed\\ntua-parkinson-dataset-master_2\\NTUA\\FINAL DATASET(PREPROCESSED)\\Model\\my_model_GA4(94.83).h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
